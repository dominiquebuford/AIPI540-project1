# -*- coding: utf-8 -*-
"""main_Inception_Vegetable

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jwsS18qdt7nShOoABnGtrZ7ejvI8BKWw
"""

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau

def train_and_finetune_model(train_dir, validation_dir, initial_lr=0.001, fine_tune_lr=0.0001, initial_epochs=15, fine_tune_epochs=15):
    # # Here we load in the InceptionV3 model from Google
    base_model = InceptionV3(weights='imagenet', include_top=False)

    # Add custom layers on top of InceptionV3
    x = base_model.output
    # Initiate the model with a GlobalAveragePooling2D layer, which flattens
    # information from the feature extraction layers
    x = GlobalAveragePooling2D()(x)
    # Stabilize the training of the neural network
    x = BatchNormalization()(x)
    # A fully connected hidden layer with 1024 neurons is introduced
    x = Dense(1024, activation='relu')(x)
    # Output layer with 15 neurons (1 for each class)
    predictions = Dense(15, activation='softmax')(x)

    # Define the model
    model = Model(inputs=base_model.input, outputs=predictions)

    # Freeze the base layers of the Transfer Learning model
    for layer in base_model.layers:
        layer.trainable = False

    # Use the Adam optimizer with a learning rate of 0.001
    # Loss is based on categorical cross entropy and measures the accuracy
    model.compile(optimizer=Adam(lr=initial_lr), loss='categorical_crossentropy', metrics=['accuracy'])

    # Increase the diversity of the training dataset
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        zoom_range=0.2,
        rotation_range=30,
        horizontal_flip=True,
        brightness_range=[0.8, 1.2],
        fill_mode='nearest'
    )
    validation_datagen = ImageDataGenerator(rescale=1./255)

    # Here we create data generators for training and validation datasets
    # using TensorFlow's Keras API
    train_generator = train_datagen.flow_from_directory(
        train_dir,
        target_size=(299, 299),
        batch_size=32,
        class_mode='categorical'
    )
    validation_generator = validation_datagen.flow_from_directory(
        validation_dir,
        target_size=(299, 299),
        batch_size=32,
        class_mode='categorical'
    )

    # ReduceLROnPlateau callback is utilized to monitor the validation loss
    # Learning rate will multiplied by a factor of 0.2 when there is no improvement of validation loss
    # If there is no improvement in the modelâ€™s validation loss after 5 epochs, the learning rate is reduced
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001, verbose=1)

    # Initial training with 15 epochs
    history = model.fit(
        train_generator,
        epochs=initial_epochs,
        validation_data=validation_generator,
        callbacks=[reduce_lr]
    )

    # Unfreeze the last 20 layers for fine-tuning
    for layer in base_model.layers[-20:]:
        layer.trainable = True

    # Recompile the model for fine-tuning
    model.compile(optimizer=Adam(lr=fine_tune_lr), loss='categorical_crossentropy', metrics=['accuracy'])

    # Fine-tune the model with adjusted epochs
    fine_tune_history = model.fit(
        train_generator,
        epochs=fine_tune_epochs,
        validation_data=validation_generator,
        callbacks=[reduce_lr]
    )

    # Visualize training and validation accuracy
    plt.plot(fine_tune_history.history['accuracy'], label='Training accuracy after fine-tuning')
    plt.plot(fine_tune_history.history['validation_accuracy'], label='Validation accuracy after fine-tuning')
    plt.title('Training and Validation Accuracy after Fine-Tuning')
    plt.legend()
    plt.show()

# Example usage
#train_and_finetune_model('/content/drive/MyDrive/Project_1/Vegetables/train', '/content/drive/MyDrive/Project_1/Vegetables/validation')